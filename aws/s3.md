# AWS S3

## Introduction

S3 stands for *Simple Storage Service*. It is an *object* storage, which means
it is a file storage but files cannot be executed like they could be on a block
storage (your hard drive is a block storage device) so you cannot install an
operating system or a database on S3.

S3 is a highly redundant and available storage, so there is very little chance
that you will lose any file and they will be accessible any time you need them.
S3 is also an unlimited storage, you can upload as much data as you want as
long as you can pay for storage management of your data.

S3 is organized as *buckets*. Buckets can be thought of as kind of folders to
store your data, but can also further organize data in folders in each bucket.
The name of a bucket must be globally unique, so when you create a bucket the
name you had in mind might have already been taken. This is because a URL is
created using the name of the bucket, and URLs must be globally unique.

Buckets are a "flat" storage space, meaning that an object name with a
slash-delimited prefix such as `uploads/myfile.ext` is not stored in a folder
named `uploads`, but `upload` becomes part of the file name. Hence, folder
navigation is virtual and simply a convenience of S3 access through the AWS console.

## Objects

An object is a file stored in S3. Each object has the following attributes:

* key (the file name)
* value (the file data)
* a version ID (you can store previous versions of your files when you update
  them, although this is not enabled by default)
* metadata (information about the object, e.g. timestamp, encryption key, etc)
* subresources (access control list, etc)

An object size cannot be larger than 5 TB (terabyte).

Object data follows two consistency models:

* read-after-write consistency when uploading a new object. The object is immediately available after upload
* eventual consistency when deleting or overwriting an object. After the delete or overwrite operation, a request on the object may return the previous version of the object

## Data availability and durability

S3 is designed for 99.99% availability, meaning that your objects will basically
be always accessible. S3 is designed for 99.9999999% (11 9s) durability, meaning that your objects will basically be never lost because of a disk failure. S3 is the place to go if you want to make sure your data is never lost.  

## Storage classes

S3 has several storage classes that come with different availability and prices. Data durability is 11 9s regardless of the storage class. From the most expensive to the cheapest, we have:

* Standard: the most expensive, availability is 99.99%
* Infrequent Access (IA): less expensive than standard, availability is
  99.9%
* Intelligent Tiering: a machine learning system figures out if your data
  should be in standard or IA storage so you can save money, availability is
  99.9%
* One Zone IA: cheaper than IA and storage is less redundant (restricted to a
  single availability zone) so availability is 99.5%. This is the storage to go
  for with non-crucial data (can be easily re-created) and the cheapest storage
* S3 Glacier and Glacier Deep Archive: this storage is for archived data that
  must not be immediately accessed. Retrieval time for Glacier is adjustable
  from several minutes to several hours, while Glacier Deep Archive comes with
  a retrieval times of up to 12 hours. These two archive storage classes are
  cheaper than the other classes, and Deep Archive is the cheapest.

S3 allows object *lifecycle management*, so objects can be moved from one
storage class to the other in order to save money. A common lifecycle policy
is:

1. initially store objects in the Intelligent Tiering class
2. move the objects from standard to Glacier after 30 days
3. delete objects from Glacier after some time (e.g. 7 years for legal reasons)

## Security

S3 data can be encrypted at rest using server-side encryption, using
Amazon-managed or customer-managed keys.

To help prevent accidental object deletion, one can enable MFA for delete
operations.

Finally, bucket and object access can be regulated using Access Control Lists
(ACLs, bucket and object level) and bucket policies (bucket level).

## Billing

When you use S3, you are charged on:

* amount of storage you use
* number of requests you make (upload, download, list, etc)
* data transfers

One good rule of thumb is that you pay for everything going in and out of S3
except in the following cases:

* data going in S3 from the internet
* data going out to an EC2 instance, if the bucket and the instance are in the
  same region
* data going out to AWS CloudFront (AWS's Content Delivery Network)

## Static web hosting

One super cool feature of S3 is that it allows you to setup a *static website*.
Static means that the data composing the website is HTML, images, CSS, and
eventually client-side scripts. Remember than S3 is an *object* storage so it
does not execute code contained in the data it stores. One can even add a
custom URL to point to your static S3 website, using AWS Route53.

Configuring a website for static web hosting is simple, and is described
[here](https://docs.aws.amazon.com/AmazonS3/latest/dev/WebSiteHosting.html).

## Read the docs

You should have a look at the [FAQ](https://aws.amazon.com/s3/faqs) to have a
high level view of AWS S3. For more details go to the [Developer Guide for
S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html).

## Using the AWS CLI for S3

If you are not familiar with the AWS command line interface (CLI), read the
[user guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html) and
follow its instructions.

### List buckets

To get the list of S3 buckets in your account, run:

```
aws s3 ls
```

### List objects in a bucket

To get the list of objects in a bucket run:

```
aws s3 ls s3://mybucket/
```

You can also provide a path within the bucket to list objects stored under a
specific path prefix:

```
aws s3 ls s3://mybucket/path-prefix/
```

### Copy object to and from S3

The syntax for the `cp` command is:

```
aws s3 cp <destination> <target>
```

To copy a file in your current directory to S3, run:

```
aws s3 cp myfile.ext s3://mybucket/path-prefix/myfile.ext
```

Conversly, you can download a file from S3 by running:

```
aws s3 sp s3://mybucket/path-prefix/myfile.ext .
```

## Using `boto3` for S3 programmatic access with Python

`boto3` is the AWS SDK for Python. We will cover only a few commands, read the [S3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html) for usage details.

Usually, you instanciate an S3 `Client` object which then allows you to call
various methods to list buckets and objects, download or upload objects, etc.
The list of `Client` methods is described [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html).

### Upload an S3 object

You can upload a local file to an S3 object:

```
import boto3

client = boto3.client("s3")
client.upload_file(
    Filename="/home/ubuntu/myfile.ext",
    Bucket="mybucket",
    Key="uploads/myfile.ext",
)
```

### Download an S3 object

You can download an S3 object to a local file:

```
import boto3

client = boto3.client("s3")
client.download_file(
    Bucket="mybucket",
    Key="uploads/myfile.ext",
    Filename="/home/ubuntu/myfile_copy.ext",
)
```

### List S3 objects

Listing S3 objects in a bucket is a slightly more involved example because AWS
returns a maximum of 1000 objects per "list" request, so the results are
paginated.

```
import boto3


client = boto3.client("s3")

kwargs = {"Bucket": "mybucket", "Prefix": "uploads"}
token = ""  # contination token that allows to fetch next page of results
keys = []  # list of object names

while token is not None:
    if token != "":
        kwargs["ContinuationToken"] = token
    response = client.list_objects_v2(**kwargs)
    contents = response.get("Contents")
    for obj in contents:
        key = obj.get("Key")
        if key[-1] != "/":  # ignore directories
            keys += key,  # extend the list of keys
    token = response.get("NextContinuationToken")
```
